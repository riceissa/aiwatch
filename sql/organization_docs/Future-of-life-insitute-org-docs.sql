insert into organization_documents(url, title, publication_date, modified_date, author, publisher, affected_organizations, affected_people, document_scope, cause_area, notes) values
   (
        'https://forum.effectivealtruism.org/posts/j5xhPbj7ywdv6aEJc/ama-future-of-life-institute-s-eu-team', /* url */
        'AMA: Future of Life Institute''s EU Team', /* title */
        '2022-01-31', /* publication_date */
        NULL, /* modified_date */
        'Risto Uuk', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'AMA', /* document_scope */
        'AI safety', /* cause_area */
        'Risto Uuk and Mark Brakel of the Future of Life Institute''s EU team discuss their work on AI safety and governance, including the institute''s AI risk mitigation efforts and their hiring of a new EU Policy Analyst.' /* notes */
    )
      ,(
        'https://forum.effectivealtruism.org/posts/44XPFrHiFwFBM2jfL/an-appraisal-of-the-future-of-life-institute-ai-existential',  /* url */
        'An appraisal of the Future of Life Institute AI existential risk program', /* title */
        '2022-12-11', /* publication_date */
        NULL, /* modified_date */
        'PabloAMC', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'The article appreciates the Future of Life Institute''s efforts in establishing an AI existential risk community and its contribution to AI safety through initiatives like the Vitalik Buterin PhD and postdoc fellowships.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/eaWSC7QXJX3sP4zry/the-future-of-life-institute-is-hiring',  /* url */
        'The Future of Life Institute is Hiring', /* title */
        '2023-09-19', /* publication_date */
        NULL, /* modified_date */
        'Palus Astra', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'Astra provides an overview of the Future of Life Institute''s recent hiring announcement for a project coordinator role, highlighting the skills and responsibilities required for the position.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/8KhGio2rhgHgsBoZ6/a-summary-of-current-work-in-ai-governance',  /* url */
        'A summary of current work in AI governance', /* title */
        '2023-06-17', /* publication_date */
        NULL, /* modified_date */
        'Constructive', /* author */
       'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides a summary of recent work in AI governance, highlighting key organizations like the Future of Life Institute, which is focused on addressing risks from advanced AI and fostering collaboration among stakeholders to ensure beneficial outcomes.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/D8NfyuQeGspM9fYpT/begging-pleading-ai-orgs-to-comment-on-nist-ai-risk',  /* url */
        'Begging, Pleading AI Orgs to Comment on NIST AI Risk Management Framework', /* title */
        '2022-12-15', /* publication_date */
        NULL, /* modified_date */
         NULL, /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article urges AI safety and alignment organizations, such as Future of Life Institute, to provide public comments on NIST''s AI Risk Management Framework to influence its development and incorporate safeguards against potential AI risks.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/wFC3axfuwABHmoQ9H/the-vitalik-buterin-fellowship-in-ai-existential-safety-is',  /* url */
        'The Vitalik Buterin Fellowship in AI Existential Safety is open for applications!', /* title */
        '2022-10-14', /* publication_date */
        NULL, /* modified_date */
        'Cynthia Chen', /* author */
       'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The Future of Life Institute is offering the 2023 Vitalik Buterin Fellowship in AI Existential Safety, providing funding for PhD and postdoctoral researchers focused on minimizing existential risks from AI.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/Htq8ucEFXp9EEiYBn/announcing-the-ai-safety-summit-talks-with-yoshua-bengio',  /* url */
        'Announcing the AI Safety Summit Talks with Yoshua Bengio', /* title */
        '2024-05-15', /* publication_date */
        NULL, /* modified_date */
        'Otto', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article announces the AI Safety Summit Talks, featuring key figures like Max Tegmark from the Future of Life Institute, aimed at addressing AI existential risks through public debate and governance discussions.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/akbwyBioGBd68CsNx/summary-the-case-for-halting-ai-development-max-tegmark-on',  /* url */
        'Summary: The Case for Halting AI Development - Max Tegmark on the Lex Fridman Podcast', /* title */
        '2023-04-17', /* publication_date */
        NULL, /* modified_date */
        'Madhav Malhotra', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Max Tegmark, cofounder of the Future of Life Institute, argues on the Lex Fridman Podcast for halting AI development beyond GPT-4 due to safety concerns and the global need for a coordinated pause to address the risks of advancing Artificial General Intelligence (AGI) too quickly.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/h95YqrLvjMovGmPna/latest-ea-updates-for-april-2019',  /* url */
        'Latest EA Updates for April 2019', /* title */
        '2019-05-01', /* publication_date */
        NULL, /* modified_date */
        'David Nash', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides an April 2019 update on effective altruism activities, covering media, organization news, grants, and research. Of note, the Future of Life Institute is highlighted for awarding Matthew Meselson, a key figure in the fight against bio-weapons, in recognition of his work.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/Ewk9eXrcRRcJvqBY8/ai-risk-and-policy-forecasts-from-metaculus-and-fli-s-ai',  /* url */
        'AI Risk & Policy Forecasts from Metaculus & FLI’s AI Pathways Workshop', /* title */
        '2023-05-16', /* publication_date */
        NULL, /* modified_date */
        'Will Aldred', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Will Aldred''s article summarizes key findings from a joint workshop by Metaculus and the Future of Life Institute, highlighting policy directions like API restrictions and model export controls as potential measures to mitigate AI risks, with insights from Pro Forecasters and AI safety experts.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/iAowzcZm87wNrTQCb/a-biosecurity-and-biorisk-reading-list',  /* url */
        'A Biosecurity and Biorisk Reading+ List', /* title */
        '2021-05-14', /* publication_date */
        NULL, /* modified_date */
        'Tessa A', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'This comprehensive biosecurity and biorisk reading list highlights essential resources, including reports and podcasts from the Future of Life Institute, aimed at reducing global catastrophic biological risks and advancing biosecurity governance.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/4CbJRAfr2JsYXQ9wD/latest-ea-updates-for-september-2019',  /* url */
        'Latest EA Updates for September 2019', /* title */
        '2019-05-28', /* publication_date */
        NULL, /* modified_date */
        'David Nash', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'This article provides a comprehensive update on various effective altruism organizations and initiatives for September 2019, highlighting topics such as grants, research, podcasts, and media coverage, including a new podcast series by the Future of Life Institute focusing on climate change risks and global security.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/AQrthFKWgJvMWw5JB/i-m-interviewing-max-tegmark-about-ai-safety-and-more-what',  /* url */
        'I''m interviewing Max Tegmark about AI safety and more. What shouId I ask him?', /* title */
        '2022-05-13', /* publication_date */
        NULL, /* modified_date */
        'Robert Wiblin', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'AMA', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses an upcoming interview with Max Tegmark, founder of the Future of Life Institute, and seeks suggestions for AI safety-related questions, focusing on his work with the Institute and advancements in AI alignment.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/kMcr72cQ78q8G2jNv/link-center-for-the-governance-of-ai-govai-annual-report',  /* url */
        '[Link] Center for the Governance of AI (GovAI) Annual Report 2018', /* title */
        '2018-12-21', /* publication_date */
        NULL, /* modified_date */
        'Markus Anderljung', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The 2018 GovAI Annual Report highlights the organization''s growth and research efforts, supported by key funders including the Future of Life Institute, focusing on AI governance to mitigate risks and ensure AI benefits humanity.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/3zoxiT6bnaTpLZZD3/fli-podcast-series-imagine-a-world-about-aspirational',  /* url */
        'FLI podcast series, "Imagine A World", about aspirational futures with AGI', /* title */
        '2018-12-21', /* publication_date */
        NULL, /* modified_date */
        'Jackson Wagner', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides an overview of the Future of Life Institute''s podcast series "Imagine A World," which explores aspirational futures shaped by AGI through discussions on innovative governance, digital nations, and inclusive AI development, all based on winning entries from FLI''s AI Worldbuilding Contest.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/3RpCB4rhpPNoCH7mL/ea-organization-updates-july-2019',  /* url */
        'EA Organization Updates: July 2019', /* title */
        '2019-08-07', /* publication_date */
        NULL, /* modified_date */
        'Aaron Gertler', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides updates from various EA organizations, with the Future of Life Institute highlighting the winner of their Sapiens Plurum short story contest and the release of podcasts on AI governance and climate risks, while preparing a series on climate risk topics based on community input.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/M4NEtSghTx9BWBDQa/ea-organization-updates-march-2020',  /* url */
        'EA Organization Updates: March 2020', /* title */
        '2020-04-17', /* publication_date */
        NULL, /* modified_date */
        'Aaron Gertler', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides updates from various effective altruism organizations in March 2020, highlighting the Future of Life Institute''s response to the COVID-19 crisis, including their involvement in government advisory roles and the development of epidemic forecasting tools.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/LFN2NtfcKaCFayLtC/fli-ai-alignment-podcast-evan-hubinger-on-inner-alignment',  /* url */
        'FLI AI Alignment podcast: Evan Hubinger on Inner Alignment, Outer Alignment, and Proposals for Building Safe Advanced AI', /* title */
        '2020-07-01', /* publication_date */
        NULL, /* modified_date */
        'Evhub', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'In a podcast episode from the Future of Life Institute, Evan Hubinger discusses critical issues in AI safety, focusing on inner and outer alignment, and proposes promising solutions to ensure the safe development of advanced AI.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/dYf4w6qvidP7x5AND/data-collection-for-ai-alignment-career-review',  /* url */
        'Data collection for AI alignment - Career review', /* title */
        '2022-07-03', /* publication_date */
        NULL, /* modified_date */
        'Benjamin Hilton|80000_Hours', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses the critical role of data collection for AI alignment, emphasizing its importance in shaping the future of AI safety and featuring insights from the Future of Life Institute and individuals like Long Ouyang, who transitioned from psychology to contribute to AI safety efforts.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/9orJx6uvgbLD7FkGC/fli-is-hiring-a-new-director-of-us-policy',  /* url */
        'FLI is hiring a new Director of US Policy', /* title */
        '2022-07-03', /* publication_date */
        NULL, /* modified_date */
        'Aaguirre', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The Future of Life Institute is seeking a new Director of US Policy to enhance its advocacy efforts in AI governance, focusing on the promotion of ethical AI policies and collaboration with key stakeholders.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/EjGowxHhRifb2r8tE/welcome-to-apply-the-2024-vitalik-buterin-fellowships-in-ai',  /* url */
        'Welcome to Apply: The 2024 Vitalik Buterin Fellowships in AI Existential Safety by FLI!', /* title */
        '2024-09-02', /* publication_date */
        NULL, /* modified_date */
        'Zhijing Jin', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The Future of Life Institute is now accepting applications for the 2024 Vitalik Buterin Fellowships in AI Existential Safety, offering funding for PhD and postdoc researchers dedicated to advancing safety in AI research.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/j5xhPbj7ywdv6aEJc/ama-future-of-life-institute-s-eu-team',  /* url */
        'AMA: Future of Life Institute''s EU Team', /* title */
        '2022-01-31', /* publication_date */
        NULL, /* modified_date */
        'Risto Uuk', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'AMA', /* document_scope */
        'AI safety', /* cause_area */
        'Risto Uuk and Mark Brakel of the Future of Life Institute''s EU team provide an overview of FLI''s policy efforts in Europe, focusing on AI safety, lethal autonomous weapons, and an open position for an EU Policy Analyst.' /* notes */
    ) 
    ,(
        'https://forum.effectivealtruism.org/posts/EjGowxHhRifb2r8tE/welcome-to-apply-the-2024-vitalik-buterin-fellowships-in-ai',  /* url */
        'The Future of Life Institute is Hiring', /* title */
        '2020-04-26', /* publication_date */
        NULL, /* modified_date */
        'Palus Astra', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'Palus Astra announces a job opening at the Future of Life Institute for a project coordinator, highlighting the organization''s focus on improving humanity''s long-term future through global collaboration.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/44XPFrHiFwFBM2jfL/an-appraisal-of-the-future-of-life-institute-ai-existential',  /* url */
        'An appraisal of the Future of Life Institute AI existential risk program', /* title */
        '2022-12-11', /* publication_date */
        NULL, /* modified_date */
        'PabloAMC', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'The article provides an appreciation of the Future of Life Institute''s efforts in fostering AI existential risk research through their academic community and the Vitalik Buterin fellowships, highlighting the program''s impact on AI safety collaboration and talent development.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/Ewk9eXrcRRcJvqBY8/ai-risk-and-policy-forecasts-from-metaculus-and-fli-s-ai',  /* url */
        'AI Risk & Policy Forecasts from Metaculus & FLI’s AI Pathways Workshop', /* title */
        '2023-05-16', /* publication_date */
        NULL, /* modified_date */
        'Will Aldred', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Will Aldred''s article discusses key insights from Metaculus and the Future of Life Institute''s AI Pathways Workshop, focusing on forecasting policy directions to reduce existential risks from advanced AI through international collaboration, API restrictions, and potential U.S. policies.' /* notes */
    )
      ,(
        'https://forum.effectivealtruism.org/posts/sBeqhuQPLFgPiHevF/fli-is-hiring-across-comms-and-ops',  /* url */
        'FLI is hiring across Comms and Ops', /* title */
        '2024-07-25', /* publication_date */
        NULL, /* modified_date */
        'Ben_Eisenpress', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'The Future of Life Institute is hiring for multiple remote roles in communications and operations, aiming to steer transformative technology away from existential risks.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/z4Z4BA2tMGbN3fSiL/2023-news-on-ai-safety-animal-welfare-global-health-and-more',  /* url */
        '2023: news on AI safety, animal welfare, global health, and more', /* title */
        '2024-01-06', /* publication_date */
        NULL, /* modified_date */
        'Lizka', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'In 2023, the Future of Life Institute played a pivotal role in AI safety by releasing a high-profile letter calling for a pause on large-scale AI experiments, garnering support from major public figures and AI experts.' /* notes */
    )
     ,(
        'https://newsletter.futureoflife.org/p/fli-newsletter-february-2024',  /* url */
        'Future of Life Institute Newsletter: FLI x The Elders, and #BanDeepfakes', /* title */
        '2024-03-04', /* publication_date */
        NULL, /* modified_date */
        'Maggie Munro', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Maggie Munro''s article in the Future of Life Institute''s newsletter highlights their partnership with The Elders to address global risks, the launch of a #BanDeepfakes campaign, and new funding opportunities to support AI research and governance.' /* notes */
    )
     ,(
        'https://futureoflife.org/document/bis-rule-for-establishment-of-reporting-requirements/',  /* url */
        'RfC on BIS Rule for ‘Establishment of Reporting Requirements for Advanced AI Models and Computing Clusters’', /* title */
        '2024-10-11', /* publication_date */
        NULL, /* modified_date */
        'Hamza Chaudhry', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Hamza Chaudhry of the Future of Life Institute provides recommendations to the Bureau of Industry and Security (BIS) regarding reporting requirements for AI models and computing clusters, focusing on enhancing safety practices and national security.' /* notes */
    )
      ,(
        'https://forum.effectivealtruism.org/posts/bd7yr3eozzzhMuKCi/what-is-the-eu-ai-act-and-why-should-you-care-about-it',  /* url */
        'What is the EU AI Act and why should you care about it?', /* title */
        '2021-09-21', /* publication_date */
        NULL, /* modified_date */
        'MathiasKB', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses the EU AI Act''s impact on future AI development, with insights from the Future of Life Institute on regulatory challenges and ethical priorities for AI governance.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/wpcFJ2CKmffQ5QWQY/ea-organization-updates-september-2019',  /* url */
        'EA Organization Updates: September 2019', /* title */
        '2019-10-10', /* publication_date */
        NULL, /* modified_date */
        'Aaron Gertler', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article outlines key activities by EA-affiliated organizations, including the Future of Life Institute''s new Not Cool podcast series, focusing on expert discussions around climate change issues.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/n5agKCYxzxiuX8f4X/what-questions-should-we-ask-speakers-at-the-stanford',  /* url */
        'What Questions Should We Ask Speakers at the Stanford Existential Risks Conference?', /* title */
        '2021-04-10', /* publication_date */
        NULL, /* modified_date */
        'Kuhanj', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'AMA', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses potential questions for speakers at Stanford''s Existential Risks Conference, focusing on guiding discussions around managing existential risks, with insights from leaders like Jaan Tallinn, co-founder of the Future of Life Institute, on prioritizing high-impact global threats.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/n5agKCYxzxiuX8f4X/what-questions-should-we-ask-speakers-at-the-stanford',  /* url */
        'Rationality, EA and being a movement', /* title */
        '2019-06-02', /* publication_date */
        NULL, /* modified_date */
        'Chris Leong', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The article explores effective altruism''s movement-building challenges, with lessons drawn from the interplay between rationality and community values. The Center for Future of Life Institute exemplifies strategic action in global risk reduction within the broader effective altruism landscape.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/Xiv8g92M4JMTFdfva/lessons-for-building-up-a-cause',  /* url */
        'Lessons for Building Up a Cause', /* title */
        '2018-02-10', /* publication_date */
        NULL, /* modified_date */
        'Evan_Gaensbauer', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'This article provides insights into strategies for building a cause, emphasizing lessons learned from organizations like the Future of Life Institute and other effective altruism-aligned groups focused on impactful growth and resilience.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/BXmp2A2eWiW34WAJG/ea-organization-updates-june-2020',  /* url */
        'EA Organization Updates: June 2020', /* title */
        '2020-07-16', /* publication_date */
        NULL, /* modified_date */
        'Aaron Gertler', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'In June 2020, the Future of Humanity Institute shared a guide for machine learning researchers on writing impact statements and announced updates in biosecurity and ethics research, including a call for a survey research contractor focused on AI governance.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/xwpYLzXQLwX4nBRuN/ea-updates-for-november-2019',  /* url */
        'EA Updates for November 2019', /* title */
        '2019-11-19', /* publication_date */
        NULL, /* modified_date */
        'DavidNash', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'The document provides a roundup of Effective Altruism (EA) updates for November 2019, covering grants, research, and podcasts from EA-aligned organizations, such as Open Philanthropy, 80,000 Hours, and the Future of Life Institute, which received a $100,000 grant for advancing AI safety research.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/gJGMFdGqFhs3mKo2s/supplement-to-the-brussels-effect-and-ai-how-eu-ai',  /* url */
        'Supplement to "The Brussels Effect and AI: How EU AI regulation will impact the global AI market"', /* title */
        '2022-08-16', /* publication_date */
        NULL, /* modified_date */
        'MarkusAnderljung', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'Markus Anderljung’s report explores how the EU’s AI Act may influence global AI governance by examining the potential for a “Brussels Effect,” where EU standards drive international AI regulation, with implications for AI safety and policy work from organizations like the Future of Life Institute.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/LFN2NtfcKaCFayLtC/fli-ai-alignment-podcast-evan-hubinger-on-inner-alignment',  /* url */
        'FLI AI Alignment podcast: Evan Hubinger on Inner Alignment, Outer Alignment, and Proposals for Building Safe Advanced AI', /* title */
        '2020-07-01', /* publication_date */
        NULL, /* modified_date */
        'Evan Hubinger', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'Evan Hubinger discusses key AI safety challenges and potential solutions, with a focus on inner and outer alignment, in the Future of Life Institute''s AI alignment podcast.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/dCb8tWsAmbYPSiqYT/military-artificial-intelligence-as-contributor-to-global',  /* url */
        'Military Artificial Intelligence as Contributor to Global Catastrophic Risk', /* title */
        '2022-06-27', /* publication_date */
        NULL, /* modified_date */
        'MMMaas| Di Cooke', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The article discusses the risks and regulatory needs of military AI to prevent global instability, with the Future of Life Institute advocating for international frameworks to ensure safety and alignment with global well-being' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/ivHfucqDNeFAR5mkH/newsletter-for-alignment-research-the-ml-safety-updates',  /* url */
        'Newsletter for Alignment Research: The ML Safety Updates', /* title */
        '2022-10-22', /* publication_date */
        NULL, /* modified_date */
        'Esben Kran| Thomas Steinthal| Sabrina Zaki| Apart Research', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The ML Safety Updates newsletter discusses recent AI safety research, including counterarguments to AI existential risk and opportunities in the field, while promoting resources like the Center for Human-Compatible AI and the Future of Life Institute for those interested in AI safety work.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/ivHfucqDNeFAR5mkH/newsletter-for-alignment-research-the-ml-safety-updates',  /* url */
        'CSER Special Issue: ''Futures of Research in Catastrophic and Existential Risk''', /* title */
        '2018-10-02', /* publication_date */
        NULL, /* modified_date */
        'Haydn Belfield', /* author */
        'Effective Altruism Forum', /* publisher */
        'Future of Life Institute', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety', /* cause_area */
        'The CSER special issue explores multidisciplinary research on catastrophic and existential risks, including contributions supported by the Future of Life Institute, focusing on governance, technology, and interventions for mitigating global threats.' /* notes */
    )
;