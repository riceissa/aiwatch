insert into organization_documents(url, title, publication_date, modified_date, author, publisher, affected_organizations, affected_people, document_scope, cause_area, notes) values
    (
        'https://forum.effectivealtruism.org/posts/59egqFgZBrfPqXWTr/ama-we-work-in-operations-at-ea-aligned-organizations-ask-us', /* url */
        'AMA: We Work in Operations at EA-aligned organizations. Ask Us Anything.', /* title */
        '2021-02-03', /* publication_date */
        NULL, /* modified_date */
        'Marisa', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'AMA', /* document_scope */
        'Effective altruism' /* cause_area */
        'This article offers an overview of operations work within EA-aligned organizations, highlighting roles like those at the Center for Human-Compatible AI, where staff manage tasks ranging from hiring and finance to external communications and event coordination.' /* notes */
    )
    ,(
        'https://www.openphilanthropy.org/blog/reflections-our-2018-generalist-research-analyst-recruiting', /* url */
        'Interview about CHAI', /* title */
        '2019-03-07', /* publication_date */
        NULL, /* modified_date */
        'Rosie Campbell', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'Rosie Campbell provides an accessible overview of the Center for Human-Compatible AI’s (CHAI) approach to ensuring AI technologies are aligned with human safety and well-being in a broad-audience interview with Mia Dand.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/iBTon2dRYwcoS9Jyr/predict-responses-to-the-existential-risk-from-ai-surveyy', /* url */
        'Predict responses to the "existential risk from AI" survey', /* title */
        '2018-03-26', /* publication_date */
        NULL, /* modified_date */
        ' Rob Bensinger', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'Rob Bensinger''s article provides an overview of a survey on existential risks from AI, examining the likelihood of future value losses due to insufficient technical AI safety research and system misalignment, with contributions from major AI organizations, including the Center for Human-Compatible AI.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/MskKEsj8nWREoMjQK/introduction-to-pragmatic-ai-safety-pragmatic-ai-safety-1', /* url */
        'Introduction to Pragmatic AI Safety [Pragmatic AI Safety #1]', /* title */
        '2022-05-19', /* publication_date */
        NULL, /* modified_date */
        'TW123|Dan H', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article introduces "Pragmatic AI Safety" as an approach focusing on practical methods to address AI risks through collaboration with ML research, minimal capability enhancements, and sociotechnical insights, with contributions from the Center for Human-Compatible AI, aiming to improve alignment and risk mitigation efforts..' /* notes */
    )
    ,(
        'https://thegradientpub.substack.com/p/stuart-russell-the-foundations-of#details', /* url */
        'Stuart Russell: The Foundations of Artificial Intelligence', /* title */
        '2019-10-06', /* publication_date */
        NULL, /* modified_date */
        'Daniel Bashir', /* author */
        'The Gradient Pub', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'In this podcast episode, Stuart Russell discusses foundational AI concepts, focusing on the Center for Human-Compatible AI’s efforts to address control challenges in developing safe, rational AI systems.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/9WxdtLEfEDJfBAruX/are-we-actually-improving-decision-making', /* url */
        'Are we actually improving decision-making?', /* title */
        '2021-02-05', /* publication_date */
        NULL, /* modified_date */
        'Remmelt', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article critiques current field-building initiatives within effective altruism, emphasizing the need for more collaboration between EA groups and diverse professional stakeholders, such as those at the Center for Human-Compatible AI, to enhance balanced decision-making and avoid potential blindspots.' /* notes */
    )
    ,(
        'https://www.vox.com/the-highlight/23447596/artificial-intelligence-agi-openai-gpt3-existential-risk-human-extinction', /* url */
        'AI experts are increasingly afraid of what they’re creating', /* title */
        '2022-11-28', /* publication_date */
        NULL, /* modified_date */
        'Kelsey Piper', /* author */
        'Vox', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article highlights the Center for Human-Compatible AI''s role in researching and mitigating the existential risks posed by advanced artificial intelligence, with an emphasis on balancing innovation and human safety.' /* notes */
    )
    ,(
        'https://humancompatible.ai/news/2024/08/07/social-choice-should-guide-ai-alignment-in-dealing-with-diverse-human-feedback/#social-choice-should-guide-ai-alignment-in-dealing-with-diverse-human-feedback', /* url */
        'Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback', /* title */
        '2024-08-07', /* publication_date */
        NULL, /* modified_date */
        'Rachel Feedman', /* author */
        'Human Compatible AI/', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'Feedman and Holliday''s paper, under the Center for Human-Compatible AI, explores how social choice theory can guide the alignment of foundation models with collective human preferences, addressing challenges in aggregating divergent human inputs to enhance AI ethics and safety.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/oEpGBqDCo2z5pbpvA/solving-alignment-isn-t-enough-for-a-flourishing-future', /* url */
        'Solving alignment isn''t enough for a flourishing future', /* title */
        '2024-02-02', /* publication_date */
        NULL, /* modified_date */
        'Mic', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article argues that ensuring a positive future with AI requires more than solving technical alignment challenges, highlighting the Center for Human-Compatible AI''s focus on advancing safety, ethics, and broader social implications.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/8CM9vZ2nnQsWJNsHx/existential-risk-from-ai-survey-results', /* url */
        'Existential risk from AI" survey results', /* title */
        '2021-06-01', /* publication_date */
        NULL, /* modified_date */
        'Rob Bensinger', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article highlights the Center for Human-Compatible AI''s role in advancing AI safety through research on human-AI collaboration, reward synthesis, and cautious agency design, emphasizing the importance of addressing existential risks while ensuring human-aligned AI development.' /* notes */
    )
     ,(
        'https://forum.effectivealtruism.org/posts/TbDnMt8KtNffNk9YL/what-is-most-important-for-your-productivity', /* url */
        'What Is Most Important For Your Productivity?', /* title */
        '2023-06-13', /* publication_date */
        NULL, /* modified_date */
        'Lynettebye', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'The article discusses productivity factors within the effective altruism community, highlighting the Center for Human-Compatible AI''s efforts in enhancing AI safety. It focuses on developing research in human-AI collaboration, reward synthesis, and cautious agency design to mitigate existential risks and promote human-aligned AI development.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/XKwiEpWRdfWo7jy7f/2017-ai-safety-literature-review-and-charity-comparison', /* url */
        '2017 AI Safety Literature Review and Charity Comparison', /* title */
        '2017-12-21', /* publication_date */
        NULL, /* modified_date */
        'Larks', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'This article reviews 2017 AI safety literature, comparing charities with a focus on lessons from the Center for Human-Compatible AI and other key organizations in the field.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/WAxSTJbdXMvFScPSa/why-i-m-donating-to-miri-this-year', /* url */
        'Why I''m donating to MIRI this year', /* title */
        '2016-12-01', /* publication_date */
        NULL, /* modified_date */
        'Owen Cotton-Barratt', /* author */
        'Effective Altruism Forum', /* publisher */
        'CHAI', /* affected_organizations */
        NULL, /* affected_people */
        'General discussion of organizational practices', /* document_scope */
        'AI safety' /* cause_area */
        'Owen Cotton-Barratt discusses his decision to donate to the Machine Intelligence Research Institute (MIRI) in 2024, emphasizing their critical work on AI safety and alignment, particularly in relation to the research at the Center for Human-Compatible AI. ' /* notes */
    )
    
;

