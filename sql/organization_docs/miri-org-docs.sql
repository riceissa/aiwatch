insert into organization_documents(url, title, publication_date, modified_date, author, publisher, affected_organizations, affected_people, document_scope, cause_area, notes) values
    (
        'https://forum.effectivealtruism.org/posts/jmbP9rwXncfa32seH/after-one-year-of-applying-for-ea-jobs-it-is-really-really#rwCRLwqywjN4Eu3E7', /* url */
        'Comment on After one year of applying for EA jobs: It is really, really hard to get hired by an EA organisation', /* title */
        '2019-02-27', /* publication_date */
        NULL, /* modified_date */
        'Aaron Gertler', /* author */
        'Effective Altruism Forum', /* publisher */
        'Centre for Effective Altruism|Open Philanthropy Project|Machine Intelligence Research Institute|Ought|Vox|AI Impacts|Center for Human-Compatible AI|Berkeley Existential Risk Initiative', /* affected_organizations */
        NULL, /* affected_people */
        'Job application experience', /* document_scope */
        'Various', /* cause_area */
        'Gertler describes his experience applying to various effective altruism-related organizations.' /* notes */
    )
    ,(
        'https://www.facebook.com/bshlgrs/posts/10215867821943197', /* url */
        'I think that every EA who is a software engineer …', /* title */
        '2019-02-27', /* publication_date */
        NULL, /* modified_date */
        'Buck Shlegeris', /* author */
        'Facebook', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'Shlegeris encourages effective altruist software engineers to apply to work at MIRI. The post also mentions that the first stage of the interview is the Triplebyte quiz.' /* notes */
    )
    ,(
        'https://www.facebook.com/bshlgrs/posts/10215832806467832', /* url */
        'MIRI is running a sequence of workshops with the …', /* title */
        '2019-02-22', /* publication_date */
        NULL, /* modified_date */
        'Buck Shlegeris', /* author */
        'Facebook', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'Shlegeris encourages people to apply to MIRI’s AI Risk for Computer Scientists workshop.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/3u8oZEEayqqjjZ7Nw/current-ai-safety-roles-for-software-engineers#wyKA2bQgLN4LTKFxk', /* url */
        'Comment on Current AI Safety Roles for Software Engineers', /* title */
        '2018-11-09', /* publication_date */
        NULL, /* modified_date */
        'Buck Shlegeris', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Hiring-related notice', /* document_scope */
        'AI safety', /* cause_area */
        'Shlegeris clarifies that MIRI would like to hire “as many people as engineers as possible”, and says MIRI is likely to “end up hiring more like ten” engineers over the next year.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/BScxwSun3K2MgpoNz/question-miri-corrigbility-agenda#o9LDEGTDMv8WNa2WA', /* url */
        'Comment on Question: MIRI Corrigbility Agenda', /* title */
        '2019-03-14', /* publication_date */
        NULL, /* modified_date */
        'Rob Bensinger', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Bensinger notes some updates that have been made to MIRI’s research guide (which was first published in 2014).' /* notes */
    )
    ,(
        'http://lesswrong.com/lw/mxj/working_at_miri_an_interview_with_malo_bourgon/', /* url */
        'Working at MIRI: An interview with Malo Bourgon', /* title */
        '2015-11-01', /* publication_date */
        NULL, /* modified_date */
        'Sören Mindermann', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'An interview with Malo Bourgon (program management analyst and generalist at MIRI) about MIRI’s current talent needs, how to get involved, application process, and donation vs marginal hires.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/cNPZJn8W8cmhTLgtd/on-the-concept-of-talent-constrained-organizations#Byxt4wZsn9GnCsdnq', /* url */
        'Comment on On the concept of “talent-constrained” organizations', /* title */
        '2014-03-17', /* publication_date */
        NULL, /* modified_date */
        'Eliezer Yudkowsky', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Yudkowsky weighs in regarding talent constraint. In a reply comment Luke Muehlhauser (executive director of MIRI at the time) also gives his opinion.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/HnC29723hm6kJT7KP/taking-ai-risk-seriously-thoughts-by-critch#eBDZzwqvkJ5xZjDMR', /* url */
        'Comment on “Taking AI Risk Seriously” (thoughts by Critch)', /* title */
        '2018-02-01', /* publication_date */
        NULL, /* modified_date */
        'Matthew Graves', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Graves (who does recruiting work at MIRI) gives his personal opinion on way to get involved in AI safety.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/WqDGJNtxNMT8fHe37/help-fund-lukeprog-at-siai', /* url */
        'Help Fund Lukeprog at SIAI', /* title */
        '2011-08-24', /* publication_date */
        NULL, /* modified_date */
        'Eliezer Yudkowsky', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'A call for donations specifically so MIRI can hire Luke Muehlhauser.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/6vXArgf9NsYxRYkpS/miri-research-guide', /* url */
        'MIRI Research Guide', /* title */
        '2014-11-07', /* publication_date */
        NULL, /* modified_date */
        'Nate Soares', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Blog post announcing the publication of a new research guide to help people become involved in MIRI’s AI safety research.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/snzFQJsNYqzPZS2nK/course-recommendations-for-friendliness-researchers', /* url */
        'Course recommendations for Friendliness researchers', /* title */
        '2013-01-09', /* publication_date */
        NULL, /* modified_date */
        'Louie Helm', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'A list of course and textbook recommendations for background material related to AI safety.' /* notes */
    )
    ,(
        'https://www.quora.com/What-is-it-like-to-do-contract-work-for-MIRI/answer/Vipul-Naik', /* url */
        'Answer to What is it like to do contract work for MIRI?', /* title */
        '2014-08-13', /* publication_date */
        NULL, /* modified_date */
        'Vipul Naik', /* author */
        'Quora', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'A review of doing contract work at MIRI (on distribution of computation and forecasting).' /* notes */
    )
    ,(
        'https://vipulnaik.com/miri/', /* url */
        'Machine Intelligence Research Institute (MIRI)', /* title */
        '2014-03-01', /* publication_date */
        '2016-08-27', /* modified_date */
        'Vipul Naik', /* author */
        'Vipul Naik', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Basic facts and timeline of Naik’s contract work at MIRI.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/jHteJiLpqZ4jDwThf/let-s-conduct-a-survey-on-the-quality-of-miri-s#6msikEDFTchrtxL4H', /* url */
        'Comment on Let’s conduct a survey on the quality of MIRI’s implementation', /* title */
        '2016-02-19', /* publication_date */
        NULL, /* modified_date */
        'Nate Soares', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute|Open Philanthropy Project', /* affected_organizations */
        'Daniel Dewey', /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Soares responds to a blog post calling for an evaluation of MIRI’s research output and strategy. Soares mentions an ongoing investigation by Open Philanthropy Project, as well as plans for “an independent evaluation of our organizational efficacy”.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/GA7ytcMeRQe5b27ge/ask-miri-anything-ama#wvq3rTkpq632pzGkF', /* url */
        'Comment on Ask MIRI Anything (AMA)', /* title */
        '2016-10-12', /* publication_date */
        NULL, /* modified_date */
        'Malo Bourgon', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Bourgon (Chief Operating Officer at MIRI) answers a question regarding number of hours worked by MIRI staff per week. Bourgon writes that “the average and median are pretty close at somewhere between 40–50 hours a week depending on the month. During crunch times some people might be working 60–100-hour weeks.” However, he notes that the amount of work that gets done and how people are feeling are more important than number of hours worked.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/GA7ytcMeRQe5b27ge/ask-miri-anything-ama#TYmE94YthSeKda6gr', /* url */
        'Comment on Ask MIRI Anything (AMA)', /* title */
        '2016-10-12', /* publication_date */
        NULL, /* modified_date */
        'Rob Bensinger', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Bensinger writes that MIRI is hoping to double the size of its research team over the next year or two.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/GA7ytcMeRQe5b27ge/ask-miri-anything-ama#v3o6bdaiT6xdB5EEn', /* url */
        'Comment on Ask MIRI Anything (AMA)', /* title */
        '2016-10-12', /* publication_date */
        NULL, /* modified_date */
        'Nate Soares', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'In response to a question, Soares writes that MIRI has decided against hiriting senior math people in a supervisor role, and also writes that MIRI is bottlenecked on technical writing ability.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/GA7ytcMeRQe5b27ge/ask-miri-anything-ama#pY5CWCmmSvwHXTun7', /* url */
        'Comment on Ask MIRI Anything (AMA)', /* title */
        '2016-10-12', /* publication_date */
        NULL, /* modified_date */
        'Malo Bourgon', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Bourgon answers a question about the differences in what researchers spend their time doing in academia vs at MIRI. He writes that in academia, faculty have non-research responsibilities like grant writing, teaching, supervision, and sitting on councils, as well as publish-or-perish incentives, which are all things that researchers at MIRI don’t have. He also writes that the operations team at MIRI takes care of many “distractions” so that the research team can focus on doing research.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/GA7ytcMeRQe5b27ge/ask-miri-anything-ama#g67SHr9xAw9ZgrRNW', /* url */
        'Comment on Ask MIRI Anything (AMA)', /* title */
        '2016-10-12', /* publication_date */
        NULL, /* modified_date */
        'Malo Bourgon', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Bourgon answers a question about MIRI’s preferences for its next few hires. He writes that MIRI’s current focus is on expanding its research team.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/cuB3GApHqLFXG36C6/i-am-nate-soares-ama#Dc2ADm3MB4fswywMS', /* url */
        'Comment on I am Nate Soares, AMA!', /* title */
        '2015-06-11', /* publication_date */
        NULL, /* modified_date */
        'Nate Soares', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Soares gives a list of metrics that MIRI uses internally to measure its own success.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/cuB3GApHqLFXG36C6/i-am-nate-soares-ama#oaLtyJg9jbdvKvR2e', /* url */
        'Comment on I am Nate Soares, AMA!', /* title */
        '2015-06-11', /* publication_date */
        NULL, /* modified_date */
        'Nate Soares', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'In response to a question, Soares writes that at the moment MIRI is talent-constrained, while noting that MIRI is taking steps to hire more researchers.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/cuB3GApHqLFXG36C6/i-am-nate-soares-ama#cFvct6HzMSp262Q6J', /* url */
        'Comment on I am Nate Soares, AMA!', /* title */
        '2015-06-11', /* publication_date */
        NULL, /* modified_date */
        'Nate Soares', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Soares notes that MIRI is going to hire a full-time office manager soon. He also writes that MIRI is looking for researchers who can write fast and well, and will look for “a person who can stay up to speed on the technical research but spend most of their time doing outreach and stewarding other researchers who are interested in doing AI alignment research”.' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/cuB3GApHqLFXG36C6/i-am-nate-soares-ama#SDADFyMgj9Gr9rWwm', /* url */
        'Comment on I am Nate Soares, AMA!', /* title */
        '2015-06-11', /* publication_date */
        NULL, /* modified_date */
        'Nate Soares', /* author */
        'Effective Altruism Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'Soares responds to a software engineer about how to get involved.' /* notes */
    )
    ,(
        'https://www.reddit.com/r/haskell/comments/als0g2/miri_is_looking_for_haskell_developers/', /* url */
        'MIRI is looking for Haskell developers', /* title */
        '2019-01-31', /* publication_date */
        NULL, /* modified_date */
        'theindigamer', /* author */
        'Reddit', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'A post on r/haskell linking to a tweet by Eliezer Yudkowsky saying MIRI is “currently looking for Haskell programmers with a good cultural fit”.' /* notes */
    )
    ,(
        'https://www.reddit.com/r/haskell/comments/a24hw7/miris_newest_recruit_edward_kmett/', /* url */
        'MIRI’s newest recruit: Edward Kmett!', /* title */
        '2018-12-01', /* publication_date */
        NULL, /* modified_date */
        'yiavin', /* author */
        'Reddit', /* publisher */
        'Machine Intelligence Research Institute|Ought', /* affected_organizations */
        'Edward Kmett|Nate Soares', /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'A post on r/haskell about Edward Kmett joining MIRI, with comments from Kmett. In comments, Kmett says that “Nate Soares came out to Boston personally, and made a very compelling argument for me going off and doing the work I’d been trying to complete solely in my evening hours full time”. Kmett also says he has “helped ought.org find at least one developer”.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/PqMT9zGrNsGJNfiFR/alignment-research-field-guide', /* url */
        'Alignment Research Field Guide', /* title */
        '2019-03-08', /* publication_date */
        NULL, /* modified_date */
        'Abram Demski', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        NULL, /* document_scope */
        'AI safety', /* cause_area */
        'A guide for getting involved in AI alignment research, written with MIRIx groups (local groups that meet up to discuss MIRI’s work or make progress on AI alignment, with financial support from MIRI) in mind.' /* notes */
    )
    ,(
        'https://agentfoundations.org/item?id=1470', /* url */
        'Why I am not currently working on the AAMLS agenda', /* title */
        '2017-05-12', /* publication_date */
        NULL, /* modified_date */
        'Jessica Taylor', /* author */
        'Intelligent Agent Foundations Forum', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        'Jessica Taylor', /* affected_people */
        'Employee departure', /* document_scope */
        'AI safety', /* cause_area */
        'Jessica Taylor, who has been working at the Machine Intelligence Research Institute (MIRI) on the Alignment for Advanced Machine Learning Systems (AAMLS) agenda, announces that she is no longer working on the agenda. Around this time, she also leaves MIRI. The other two people working on the agenda also leave at around the same time: Patrick LaVictoire leaves MIRI completely, and Andrew Critch goes on indefinite leave to work on the Center for Human-Compatible AI (CHAI). The connection between Taylor''s post and the departures is made in the MIRI blog post https://intelligence.org/2017/07/04/updates-to-the-research-team-and-a-major-donation/ on July 4, 2017' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/5syG88rmW5iD9kTM5/is-it-harder-to-become-a-miri-mathematician-in-2019-compared', /* url */
        'Is it harder to become a MIRI mathematician in 2019 compared to in 2013?', /* title */
        '2019-10-29', /* publication_date */
        NULL, /* modified_date */
        'Issa Rice', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        'Nate Soares', /* affected_people */
        'Third-party commentary on organization', /* document_scope */
        'AI safety', /* cause_area */
        'Issa Rice divides MIRI research employees between "mathematicians" and "engineers" and notes that recently, MIRI has hired mostly engineers and not mathematicians. He also considers the example of Nate Soares, whose background prior to joining MIRI matched the engineer profile, but who still joined and did initial work as a mathematician. His post asks the question of whether this suggests it is harder to become a MIRI mathematician in 2019 (the time of writing the post) compared to 2013. The post includes a list of potential differences between the time periods.' /* notes */
    )
    ,(
        'https://www.lesswrong.com/posts/ptmmK9PWgYTuWToaZ/what-i-ll-be-doing-at-miri', /* url */
        'What I’ll be doing at MIRI', /* title */
        '2019-11-12', /* publication_date */
        NULL, /* modified_date */
        'Evan Hubinger', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute|OpenAI', /* affected_organizations */
        'Evan Hubinger|Paul Christiano|Nate Soares', /* affected_people */
        'Successful hire', /* document_scope */
        'AI safety', /* cause_area */
        'Evan Hubinger, who has just finished an internship at OpenAI with Paul Christiano and others, is going to start work at MIRI. His research will be focused on solving inner alignment for amplification. Although MIRI''s research policy is one of nondisclosure-by-default https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/ Hubinger expects that his own research will be published openly, and that he will continue collaborating with researchers at institutions like OpenAI, Ought, CHAI, DeepMind, FHI, etc. In a comment, MIRI Executive Director Nate Soares clarifies that "my view of MIRI''s nondisclosed-by-default policy is that if all researchers involved with a research program think it should obviously be public then it should obviously be public, and that doesn''t require a bunch of bureaucracy. [...] the policy is there to enable researchers, not to annoy them and make them jump through hoops." Cross-posted from the AI Alignment Forum; original is at https://alignmentforum.org/posts/ptmmK9PWgYTuWToaZ/what-i-ll-be-doing-at-miri' /* notes */
    )
    ,(
        'https://forum.effectivealtruism.org/posts/tDk57GhrdK54TWzPY/i-m-buck-shlegeris-i-do-research-and-outreach-at-miri-ama', /* url */
        'I''m Buck Shlegeris, I do research and outreach at MIRI, AMA', /* title */
        '2019-11-15', /* publication_date */
        NULL, /* modified_date */
        'Buck Shlegeris', /* author */
        'LessWrong', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        'Buck Shlegeris', /* affected_people */
        'AMA', /* document_scope */
        'AI safety', /* cause_area */
        'On Friday November 15, Buck Shlegeris posts an Ask Me Anything (AMA) on the Effective Altruism Forum. He responds to the questions on Tuesday November 19' /* notes */
    )
,(
        'https://www.facebook.com/robbensinger/posts/10163981893970447', /* url */
        'MIRI, the place where I work, is very seriously considering moving to a different country soon …', /* title */
        '2020-10-09', /* publication_date */
        NULL, /* modified_date */
        'Rob Bensinger', /* author */
        'Facebook', /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        NULL, /* affected_people */
        'Organization operations', /* document_scope */
        'AI safety', /* cause_area */
        'Rob Bensinger, the communications person at MIRI, posts on his personal Facebook wall saying that MIRI is internally actively discussing and seriously considering a move from the San Francisco Bay Area (where MIRI is currently located) to another country or another part of the US. The post does not state the reasons MIRI is considering a move; in comments, Bensinger clarifies that it is too early to summarize the reasons MIRI is considering a move, but that he is looking for people’s thoughts on what the most important considerations are. Bensinger also posts a similar comment on LessWrong https://www.lesswrong.com/posts/FghubkDy6Dp6mnxk7/the-rationalist-community-s-location-problem?commentId=WggEETNGjd6d2XRgR' /* notes */
    )
    ,(
        'https://www.facebook.com/brianfinifter/posts/10102058260531588?comment_id=10102058311110228', /* url */
        NULL, /* title */
        '2020-06-02', /* publication_date */
        NULL, /* modified_date */
        'Brian Finifter', /* author */
        NULL, /* publisher */
        'Machine Intelligence Research Institute', /* affected_organizations */
        'Michael Anissimov', /* affected_people */
        'HR controversy', /* document_scope */
        'AI safety', /* cause_area */
        'Brian Finifter writes on his Facebook page revealing posts by Russian American Michael Anissimov (who has many Facebook connections with people at MIRI) endorsing white nationalism. Finifter calls for the transhumanist community not to tolerate such position. Among the people reacting to the Finifter publication, Eliezer Yudkowsky responds by giving more details and indicating that Anissimov no longer has a role in the community.' /* notes */
    );
